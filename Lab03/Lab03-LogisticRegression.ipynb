{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03: Logistic Regression.\n",
    "\n",
    "- Student ID: 21127113\n",
    "- Student name: Dinh Duong Hai Nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) <class 'pandas.core.frame.DataFrame'>\n",
      "(70000,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, type(X))\n",
    "print(y.shape, type(y))\n",
    "\n",
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
    "\n",
    "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - **that is the average value of a pixel in an image for the different digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "class           \n",
      "0      44.177405\n",
      "1      19.406802\n",
      "2      38.034208\n",
      "3      36.154209\n",
      "4      30.996000\n",
      "5      32.950159\n",
      "6      35.234865\n",
      "7      29.217987\n",
      "8      38.397901\n",
      "9      31.359408\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(y)\n",
    "# print(labels)\n",
    "n_label = np.unique(y).shape[0]\n",
    "\n",
    "# array stores average intensity for each label\n",
    "l_means = X.mean(axis=1)\n",
    "l_means = pd.concat([l_means, y], axis=1).groupby('class').mean()\n",
    "\n",
    "# TODO compute average intensity for each label\n",
    "print(l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average intensity using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,l_means.iloc[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n",
      "[35.10841837 39.6619898  24.7997449  ... 37.28443878 33.87627551\n",
      " 53.35841837]\n"
     ]
    }
   ],
   "source": [
    "#TODO compute average intensity for each data sample\n",
    "intensity = X_np.mean(axis=1)\n",
    "print(intensity.shape)\n",
    "print(intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people really do not know what are they doing. I am not an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_symmetry_grayscale(image):\n",
    "    # Assuming 'image' is a 1D NumPy array representing the grayscale image\n",
    "\n",
    "    # Create a mirrored version of the image\n",
    "    mirrored_image = np.flip(image)\n",
    "\n",
    "    # Calculate pixel-wise symmetry\n",
    "    symmetry_score = np.sum(np.abs(image - mirrored_image))\n",
    "\n",
    "    # Normalize symmetry score\n",
    "    symmetry = 1 - (symmetry_score / (2 * np.sum(np.abs(image))))  \n",
    "    return symmetry\n",
    "\n",
    "symmetry = np.apply_along_axis(calculate_symmetry_grayscale, axis=1, arr=X_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this feature work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0\n",
      "class          \n",
      "0      0.485135\n",
      "1      0.539071\n",
      "2      0.399001\n",
      "3      0.403066\n",
      "4      0.407103\n",
      "5      0.363520\n",
      "6      0.368320\n",
      "7      0.262586\n",
      "8      0.461888\n",
      "9      0.406346\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Symmertry by labels\n",
    "symmetry_plt = pd.concat([pd.DataFrame(symmetry), y], axis=1).groupby('class').mean()\n",
    "\n",
    "print(symmetry_plt)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,symmetry_plt.iloc[:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n",
      "[[35.10841837  0.50514078]\n",
      " [39.6619898   0.47969127]\n",
      " [24.7997449   0.12292342]\n",
      " ...\n",
      " [37.28443878  0.46922787]\n",
      " [33.87627551  0.39263526]\n",
      " [53.35841837  0.43587598]]\n"
     ]
    }
   ],
   "source": [
    "#TODO create X_new by horizontal stack intensity and symmetry\n",
    "X_new = np.hstack((intensity[:, np.newaxis], symmetry[:, np.newaxis]))\n",
    "\n",
    "print(X_new.shape) #it should be (70000,2)\n",
    "print(X_new) #it should be (70000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "First normalize data using Z-score normalization\n",
    "#### TODO: Study about Z-score normalization  \n",
    "Z-score normalization, also known as standardization, is a statistical method used to rescale and center the values of a distribution by subtracting the mean and dividing by the standard deviation. This process transforms the data into a standard normal distribution with a mean of 0 and a standard deviation of 1. The formula for Z-score normalization for a data point x is given by:\n",
    "$$\n",
    "Z = \\frac{(x−μ)}{σ}\n",
    "$$\n",
    "where:\n",
    "- $Z$ is the Z-score.\n",
    "- $x$ is the individual data point.\n",
    "- $μ$ is the mean of the dataset.\n",
    "- $σ$ is the standard deviation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Why should we normalize data?\n",
    "Normalizing data is a common preprocessing step in data analysis, machine learning, and statistical modeling. Here are several reasons why data normalization is important:\n",
    "\n",
    "1. **Equalize Scales:**\n",
    "   - Different features in a dataset might have different scales. For example, one feature may range from 0 to 100, while another ranges from 0 to 0.1. Normalizing the data ensures that all features have the same scale, preventing some features from dominating others in algorithms or analyses.\n",
    "\n",
    "2. **Facilitate Convergence in Optimization:**\n",
    "   - In optimization algorithms, such as gradient descent, normalizing features can help the algorithm converge faster. It aids in reaching the minimum or maximum of the cost function more efficiently by ensuring that the optimization process is not dominated by features with larger scales.\n",
    "\n",
    "3. **Improve Interpretability:**\n",
    "   - Normalized data can be easier to interpret. The coefficients in linear models, for example, represent the change in the target variable for a one-unit change in the corresponding feature. If features have different scales, the coefficients may not be directly comparable.\n",
    "\n",
    "4. **Enhance Model Performance:**\n",
    "   - Many machine learning algorithms, especially those based on distance metrics (e.g., k-nearest neighbors), are sensitive to the scale of features. Normalization can lead to better performance for these algorithms by preventing certain features from having a disproportionate influence.\n",
    "\n",
    "5. **Facilitate Comparisons:**\n",
    "   - When comparing or combining datasets, it's essential that the data be on a similar scale. Normalization allows for a fair comparison between datasets and facilitates the combination of information from different sources.\n",
    "\n",
    "6. **Avoid Numerical Instabilities:**\n",
    "   - In some mathematical computations, especially those involving inversion of matrices or computation of derivatives, extreme values or large scales can lead to numerical instability. Normalizing the data can mitigate such issues.\n",
    "\n",
    "7. **Prevent Overfitting:**\n",
    "   - In machine learning models, normalization can help prevent overfitting. Regularization techniques penalize large coefficients, and without normalization, features with larger scales might be penalized more, leading to a less effective model.\n",
    "\n",
    "8. **Handle Outliers:**\n",
    "   - Normalization can make a dataset more robust to outliers. If the data contains extreme values, normalizing the features can reduce the impact of outliers on the overall model.\n",
    "\n",
    "9. **Ease of Interpretation and Communication:**\n",
    "   - Normalizing data can simplify the interpretation and communication of results, making it easier for stakeholders to understand and act upon the information provided.\n",
    "\n",
    "In summary, normalizing data is a crucial step in data preprocessing to ensure that the data is suitable for analysis, modeling, and interpretation. It addresses issues related to different scales, improves the performance of certain algorithms, and facilitates meaningful comparisons between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15575488  0.5942758 ]\n",
      " [ 0.56751688  0.43379815]\n",
      " [-0.77641894 -1.81588213]\n",
      " ...\n",
      " [ 0.35252406  0.36781885]\n",
      " [ 0.04433692 -0.11515315]\n",
      " [ 1.8060324   0.15751097]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: normalize X_new\n",
    "def normalize(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "X_new = normalize(X_new)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 5)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
    "\n",
    "y_new = np.array(y).astype(int)\n",
    "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
    "y_new = y_new.reshape(-1,1)\n",
    "print (X_new.shape)\n",
    "print (y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46667, 5)\n",
      "(46667, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "def sigmoid_deriv(x):\n",
    "    '''compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input ‘x‘ has already been passed through the sigmoid\n",
    "    activation function'''\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(W, X):\n",
    "    \"\"\"\n",
    "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
    "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    return sigmoid_activation(X.dot(W))\n",
    "def predict(W, X):\n",
    " \n",
    "    '''Take the dot product between our features and weight matrix, \n",
    "       then pass this value through our sigmoid activation'''\n",
    "    #........\n",
    "    preds=sigmoid_activation(X.dot(W))\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds <= 0.5] = 0\n",
    "    preds[preds > 0] = 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function: Average negative log likelihood**\n",
    "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
    "\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(train_X, error):\n",
    "    \"\"\"\n",
    "    This is the gradient descent update of \"average negative log likelihood\" loss function. \n",
    "    In lab02 our loss function is \"sum squared error\".\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "    gradient = train_X.T.dot(error) / train_X.shape[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
    "    for epoch in np.arange(0, num_epochs):\n",
    "        h=compute_h(W,train_X)\n",
    "        error = h - train_y\n",
    "        loss = np.mean(- train_y * np.log(h) - (1 - train_y) * np.log(1 - h))\n",
    "        losses.append(loss)\n",
    "        gradient=compute_gradient(h, error)\n",
    "        W += -learning_rate * gradient\n",
    "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, loss 0.390\n",
      "Epoch 2000, loss 0.399\n",
      "Epoch 3000, loss 0.422\n",
      "Epoch 4000, loss 0.446\n",
      "Epoch 5000, loss 0.471\n",
      "Epoch 6000, loss 0.495\n",
      "Epoch 7000, loss 0.519\n",
      "Epoch 8000, loss 0.543\n",
      "Epoch 9000, loss 0.566\n",
      "Epoch 10000, loss 0.589\n",
      "Epoch 11000, loss 0.612\n",
      "Epoch 12000, loss 0.634\n",
      "Epoch 13000, loss 0.656\n",
      "Epoch 14000, loss 0.678\n",
      "Epoch 15000, loss 0.699\n",
      "Epoch 16000, loss 0.720\n",
      "Epoch 17000, loss 0.742\n",
      "Epoch 18000, loss 0.763\n",
      "Epoch 19000, loss 0.784\n",
      "Epoch 20000, loss 0.805\n",
      "Epoch 21000, loss 0.825\n",
      "Epoch 22000, loss 0.846\n",
      "Epoch 23000, loss 0.867\n",
      "Epoch 24000, loss 0.887\n",
      "Epoch 25000, loss 0.908\n",
      "Epoch 26000, loss 0.929\n",
      "Epoch 27000, loss 0.949\n",
      "Epoch 28000, loss 0.970\n",
      "Epoch 29000, loss 0.990\n",
      "Epoch 30000, loss 1.011\n",
      "Epoch 31000, loss 1.031\n",
      "Epoch 32000, loss 1.052\n",
      "Epoch 33000, loss 1.072\n",
      "Epoch 34000, loss 1.093\n",
      "Epoch 35000, loss 1.113\n",
      "Epoch 36000, loss 1.134\n",
      "Epoch 37000, loss 1.155\n",
      "Epoch 38000, loss 1.175\n",
      "Epoch 39000, loss 1.196\n",
      "Epoch 40000, loss 1.217\n",
      "==================================================\n",
      "Train err of final w:  11.734201898557869\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(train_X.shape[1], 1)\n",
    "losses=[]\n",
    "num_epochs=40000\n",
    "learning_rate=0.01\n",
    "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
    "x_preds=predict(W ,train_X)\n",
    "train_err = np.mean(x_preds != train_y) * 100\n",
    "print ('=' * 50)\n",
    "print ('Train err of final w: ', train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.94     41423\n",
      "           1       0.39      0.08      0.13      5244\n",
      "\n",
      "    accuracy                           0.88     46667\n",
      "   macro avg       0.64      0.53      0.53     46667\n",
      "weighted avg       0.84      0.88      0.85     46667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, train_X)\n",
    "print(classification_report(train_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94     20700\n",
      "           1       0.42      0.08      0.13      2633\n",
      "\n",
      "    accuracy                           0.88     23333\n",
      "   macro avg       0.66      0.53      0.53     23333\n",
      "weighted avg       0.84      0.88      0.85     23333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, test_X)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Comment on the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "### [pd.Dataframe.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
